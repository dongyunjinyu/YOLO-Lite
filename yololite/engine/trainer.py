# YOLO-Lite ğŸš€

import gc
import math
import random
import time
import warnings
from copy import copy, deepcopy
from datetime import datetime
from pathlib import Path
import numpy as np
import torch
from torch import nn, optim
from yololite.data import build_dataloader, build_yolo_dataset
from yololite.engine.validator import DetectionValidator
from yololite.nn.tasks import DetectionModel
from yololite.utils.plotting import plot_images, plot_labels, plot_results
from yololite.cfg import get_cfg, get_save_dir
from yololite.data.utils import check_det_dataset
from yololite.nn.tasks import attempt_load_one_weight, attempt_load_weights
from yololite.utils import (
    DEFAULT_CFG,
    LOGGER,
    TQDM,
    colorstr,
    yaml_save,
)
from yololite.utils.checks import check_imgsz, print_args
from yololite.utils.files import get_latest_run
from yololite.utils.torch_utils import (
    TORCH_2_4,
    de_parallel,
    EarlyStopping,
    ModelEMA,
    autocast,
    convert_optimizer_state_dict_to_fp16,
    init_seeds,
    one_cycle,
    select_device,
    strip_optimizer
)

class DetectionTrainer:
    """
    å±æ€§ï¼š
        args (SimpleNamespace): è®­ç»ƒå™¨çš„é…ç½®å‚æ•°ã€‚
        validator (BaseValidator): éªŒè¯å™¨å®ä¾‹ã€‚
        model (nn.Module): æ¨¡å‹å®ä¾‹ã€‚
        save_dir (Path): ä¿å­˜ç»“æœçš„ç›®å½•ã€‚
        wdir (Path): ä¿å­˜æƒé‡çš„ç›®å½•ã€‚
        last (Path): æœ€è¿‘æ£€æŸ¥ç‚¹çš„è·¯å¾„ã€‚
        best (Path): æœ€ä½³æ£€æŸ¥ç‚¹çš„è·¯å¾„ã€‚
        save_period (int): æ¯ x ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ï¼ˆå°äº 1 æ—¶ç¦ç”¨ï¼‰ã€‚
        batch_size (int): è®­ç»ƒçš„æ‰¹é‡å¤§å°ã€‚
        epochs (int): è®­ç»ƒçš„æ€» epoch æ•°ã€‚
        start_epoch (int): è®­ç»ƒå¼€å§‹çš„ epoch æ•°ã€‚
        device (torch.device): è®­ç»ƒä½¿ç”¨çš„è®¾å¤‡ã€‚
        amp (bool): æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰çš„æ ‡å¿—ã€‚
        scaler (amp.GradScaler): ç”¨äº AMP çš„æ¢¯åº¦ç¼©æ”¾å™¨ã€‚
        data (str): æ•°æ®çš„è·¯å¾„ã€‚
        trainset (torch.utils.data.Dataset): è®­ç»ƒæ•°æ®é›†ã€‚
        testset (torch.utils.data.Dataset): æµ‹è¯•æ•°æ®é›†ã€‚
        ema (nn.Module): æ¨¡å‹çš„ EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰ã€‚
        resume (bool): æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚
        lf (nn.Module): æŸå¤±å‡½æ•°ã€‚
        scheduler (torch.optim.lr_scheduler._LRScheduler): å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
        best_fitness (float): è¾¾åˆ°çš„æœ€ä½³é€‚åº”åº¦å€¼ã€‚
        fitness (float): å½“å‰é€‚åº”åº¦å€¼ã€‚
        loss (float): å½“å‰æŸå¤±å€¼ã€‚
        tloss (float): æ€»æŸå¤±å€¼ã€‚
        loss_names (list): æŸå¤±åç§°åˆ—è¡¨ã€‚
        csv (Path): ç»“æœ CSV æ–‡ä»¶çš„è·¯å¾„ã€‚
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None):
        """
        å‚æ•°ï¼š
            cfg (str, optional): é…ç½®æ–‡ä»¶çš„è·¯å¾„ã€‚é»˜è®¤ä¸º DEFAULT_CFGã€‚
            overrides (dict, optional): é…ç½®è¦†ç›–é¡¹ã€‚é»˜è®¤ä¸º Noneã€‚
        """
        self.args = get_cfg(cfg, overrides)  # åŠ è½½é…ç½®
        self.check_resume(overrides)  # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤è®­ç»ƒ
        self.device = select_device(self.args.device, self.args.batch)  # é€‰æ‹©è®¾å¤‡
        self.validator = None  # éªŒè¯å™¨åˆå§‹åŒ–
        self.metrics = None  # è¯„ä¼°æŒ‡æ ‡åˆå§‹åŒ–
        self.plots = {}  # ç»˜å›¾å­—å…¸åˆå§‹åŒ–
        init_seeds(self.args.seed + 1, deterministic=self.args.deterministic)  # åˆå§‹åŒ–éšæœºç§å­

        # ç›®å½•åˆå§‹åŒ–
        self.save_dir = get_save_dir(self.args)  # è·å–ä¿å­˜ç›®å½•
        self.args.name = self.save_dir.name  # æ›´æ–°æ—¥å¿—ç”¨çš„åç§°
        self.wdir = self.save_dir / "weights"  # æƒé‡ç›®å½•
        self.wdir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•
        self.args.save_dir = str(self.save_dir)  # ä¿å­˜ç›®å½•å­—ç¬¦ä¸²
        yaml_save(self.save_dir / "args.yaml", vars(self.args))  # ä¿å­˜è¿è¡Œå‚æ•°
        self.last, self.best = self.wdir / "last.pt", self.wdir / "best.pt"  # æ£€æŸ¥ç‚¹è·¯å¾„
        self.save_period = self.args.save_period  # ä¿å­˜å‘¨æœŸ

        # è®­ç»ƒå‚æ•°
        self.batch_size = self.args.batch  # æ‰¹é‡å¤§å°
        self.epochs = self.args.epochs or 100  # è®­ç»ƒçš„ epoch æ•°
        self.start_epoch = 0  # å¼€å§‹çš„ epoch
        print_args(vars(self.args))  # æ‰“å°å‚æ•°

        # è®¾å¤‡è®¾ç½®
        if self.device.type in {"cpu", "mps"}:
            self.args.workers = 0  # ä½¿ç”¨ CPU æ—¶è®¾ç½®å·¥ä½œçº¿ç¨‹ä¸º 0

        # æ¨¡å‹å’Œæ•°æ®é›†åˆå§‹åŒ–
        self.model = self.args.model  # æ¨¡å‹åˆå§‹åŒ–
        self.trainset, self.testset = self.get_dataset()  # è·å–æ•°æ®é›†
        self.ema = None  # EMA åˆå§‹åŒ–

        # ä¼˜åŒ–å·¥å…·åˆå§‹åŒ–
        self.lf = None  # æŸå¤±å‡½æ•°åˆå§‹åŒ–
        self.scheduler = None  # å­¦ä¹ ç‡è°ƒåº¦å™¨åˆå§‹åŒ–

        # epoch çº§åˆ«çš„æŒ‡æ ‡
        self.best_fitness = None  # æœ€ä½³é€‚åº”åº¦
        self.fitness = None  # å½“å‰é€‚åº”åº¦
        self.loss = None  # å½“å‰æŸå¤±
        self.tloss = None  # æ€»æŸå¤±
        self.loss_names = ["Loss"]  # æŸå¤±åç§°
        self.csv = self.save_dir / "results.csv"  # ç»“æœ CSV æ–‡ä»¶è·¯å¾„
        self.plot_idx = [0, 1, 2]  # ç»˜å›¾ç´¢å¼•

    def _setup_scheduler(self):
        """åˆå§‹åŒ–è®­ç»ƒå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚"""
        if self.args.cos_lr:
            self.lf = one_cycle(1, self.args.lrf, self.epochs)  # ä½™å¼¦è°ƒåº¦
        else:
            self.lf = lambda x: max(1 - x / self.epochs, 0) * (1.0 - self.args.lrf) + self.args.lrf  # çº¿æ€§è°ƒåº¦
        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.lf)  # å­¦ä¹ ç‡è°ƒåº¦å™¨

    def _setup_train(self):
        """æ„å»ºæ•°æ®åŠ è½½å™¨å’Œä¼˜åŒ–å™¨ã€‚"""
        # æ¨¡å‹è®¾ç½®
        ckpt = self.setup_model()  # è®¾ç½®æ¨¡å‹
        self.model = self.model.to(self.device)  # å°†æ¨¡å‹ç§»è‡³è®¾å¤‡
        self.set_model_attributes()  # è®¾ç½®æ¨¡å‹å±æ€§

        # å†»ç»“å±‚è®¾ç½®
        freeze_list = (
            self.args.freeze
            if isinstance(self.args.freeze, list)
            else range(self.args.freeze)
            if isinstance(self.args.freeze, int)
            else []
        )
        always_freeze_names = [".dfl"]  # å§‹ç»ˆå†»ç»“çš„å±‚
        freeze_layer_names = [f"model.{x}." for x in freeze_list] + always_freeze_names
        for k, v in self.model.named_parameters():
            if any(x in k for x in freeze_layer_names):
                LOGGER.info(f"å†»ç»“å±‚ '{k}'")
                v.requires_grad = False  # å†»ç»“å‚æ•°
            elif not v.requires_grad and v.dtype.is_floating_point:  # ä»…æµ®ç‚¹ç±»å‹å¯ä»¥è¦æ±‚æ¢¯åº¦
                LOGGER.info(
                    f"è­¦å‘Š âš ï¸ è®¾ç½® 'requires_grad=True' ä¸ºå†»ç»“å±‚ '{k}'ã€‚"
                )
                v.requires_grad = True

        # æ£€æŸ¥ AMP
        self.amp = torch.tensor(self.args.amp).to(self.device)  # æ˜¯å¦å¯ç”¨ AMP
        if self.amp:  # å• GPU
            self.model = self.model.to(self.device)  # ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
        self.amp = bool(self.amp)  # è½¬ä¸ºå¸ƒå°”å€¼
        self.scaler = (
            torch.amp.GradScaler("cuda", enabled=self.amp) if TORCH_2_4 else torch.cuda.amp.GradScaler(enabled=self.amp)
        )

        # æ£€æŸ¥å›¾åƒå¤§å°
        gs = max(int(self.model.stride.max() if hasattr(self.model, "stride") else 32), 32)  # ç½‘æ ¼å¤§å°
        self.args.imgsz = check_imgsz(self.args.imgsz, stride=gs, floor=gs, max_dim=1)  # æ£€æŸ¥å›¾åƒå¤§å°
        self.stride = gs  # ç”¨äºå¤šå°ºåº¦è®­ç»ƒ

        # æ•°æ®åŠ è½½å™¨
        batch_size = self.batch_size
        self.train_loader = self.get_dataloader(self.trainset, batch_size=batch_size, mode="train")

        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        self.test_loader = self.get_dataloader(self.testset, batch_size=batch_size * 2, mode="val")
        self.validator = self.get_validator()  # è·å–éªŒè¯å™¨
        metric_keys = self.validator.metrics.keys + self.label_loss_items(prefix="val")  # æŒ‡æ ‡é”®
        self.metrics = dict(zip(metric_keys, [0] * len(metric_keys)))  # åˆå§‹åŒ–æŒ‡æ ‡
        self.ema = ModelEMA(self.model)  # åˆå§‹åŒ– EMA
        if self.args.plots:
            self.plot_training_labels()  # ç»˜åˆ¶è®­ç»ƒæ ‡ç­¾

        # ä¼˜åŒ–å™¨è®¾ç½®
        self.accumulate = max(round(self.args.nbs / self.batch_size), 1)  # ä¼˜åŒ–å‰ç´¯ç§¯æŸå¤±
        weight_decay = self.args.weight_decay * self.batch_size * self.accumulate / self.args.nbs  # æƒé‡è¡°å‡
        iterations = math.ceil(len(self.train_loader.dataset) / max(self.batch_size, self.args.nbs)) * self.epochs  # è¿­ä»£æ¬¡æ•°
        self.optimizer = self.build_optimizer(
            model=self.model,
            name=self.args.optimizer,
            lr=self.args.lr0,
            momentum=self.args.momentum,
            decay=weight_decay,
            iterations=iterations,
        )
        # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        self._setup_scheduler()
        self.stopper, self.stop = EarlyStopping(patience=self.args.patience), False  # æå‰åœæ­¢
        self.resume_training(ckpt)  # æ¢å¤è®­ç»ƒ
        self.scheduler.last_epoch = self.start_epoch - 1  # ä¸ç§»åŠ¨

    def train(self):
        self._setup_train()  # è®¾ç½®è®­ç»ƒ
        nb = len(self.train_loader)  # æ‰¹æ¬¡æ•°
        nw = max(round(self.args.warmup_epochs * nb), 100) if self.args.warmup_epochs > 0 else -1  # çƒ­èº«è¿­ä»£
        last_opt_step = -1  # æœ€åä¼˜åŒ–æ­¥æ•°
        self.epoch_time = 0  # epoch æ—¶é—´
        self.epoch_time_start = time.time()  # epoch å¼€å§‹æ—¶é—´
        self.train_time_start = time.time()  # è®­ç»ƒå¼€å§‹æ—¶é—´
        LOGGER.info(
            f'å›¾åƒå¤§å° {self.args.imgsz} è®­ç»ƒ, {self.args.imgsz} éªŒè¯\n'
            f"å°†ç»“æœè®°å½•åˆ° {colorstr('bold', self.save_dir)}\n"
            f'å¼€å§‹è®­ç»ƒ ' + (f"{self.args.time} å°æ—¶..." if self.args.time else f"{self.epochs} ä¸ªå‘¨æœŸ...")
        )
        if self.args.close_mosaic:
            base_idx = (self.epochs - self.args.close_mosaic) * nb
            self.plot_idx.extend([base_idx, base_idx + 1, base_idx + 2])  # æ›´æ–°ç»˜å›¾ç´¢å¼•
        epoch = self.start_epoch
        self.optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
        while True:
            self.epoch = epoch
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")  # å¿½ç•¥è­¦å‘Š
                self.scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡

            self.model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
            pbar = enumerate(self.train_loader)  # è®­ç»ƒè¿›åº¦æ¡
            # æ›´æ–°æ•°æ®åŠ è½½å™¨å±æ€§ï¼ˆå¯é€‰ï¼‰
            if epoch == (self.epochs - self.args.close_mosaic):
                self._close_dataloader_mosaic()  # å…³é—­é©¬èµ›å…‹å¢å¼º
                self.train_loader.reset()  # é‡ç½®è®­ç»ƒåŠ è½½å™¨

            LOGGER.info(self.progress_string())  # æ‰“å°è¿›åº¦ä¿¡æ¯
            pbar = TQDM(enumerate(self.train_loader), total=nb)  # è¿›åº¦æ¡æ˜¾ç¤º
            self.tloss = None  # æ€»æŸå¤±åˆå§‹åŒ–
            for i, batch in pbar:
                # çƒ­èº«
                ni = i + nb * epoch  # å½“å‰è¿­ä»£æ¬¡æ•°
                if ni <= nw:
                    xi = [0, nw]  # x æ’å€¼
                    self.accumulate = max(1, int(np.interp(ni, xi, [1, self.args.nbs / self.batch_size]).round()))  # æ›´æ–°ç´¯ç§¯å‚æ•°
                    for j, x in enumerate(self.optimizer.param_groups):
                        # å­¦ä¹ ç‡è°ƒæ•´
                        x["lr"] = np.interp(
                            ni, xi, [self.args.warmup_bias_lr if j == 0 else 0.0, x["initial_lr"] * self.lf(epoch)]
                        )
                        if "momentum" in x:
                            x["momentum"] = np.interp(ni, xi, [self.args.warmup_momentum, self.args.momentum])

                # å‰å‘ä¼ æ’­
                with autocast(self.amp):  # è‡ªåŠ¨æ··åˆç²¾åº¦
                    batch = self.preprocess_batch(batch)  # é¢„å¤„ç†æ‰¹æ¬¡
                    self.loss, self.loss_items = self.model(batch)  # è®¡ç®—æŸå¤±
                    self.tloss = (
                        (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None else self.loss_items
                    )

                # åå‘ä¼ æ’­
                self.scaler.scale(self.loss).backward()  # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­

                # ä¼˜åŒ–
                if ni - last_opt_step >= self.accumulate:  # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ä¼˜åŒ–æ¡ä»¶
                    self.optimizer_step()  # æ‰§è¡Œä¼˜åŒ–æ­¥
                    last_opt_step = ni  # æ›´æ–°æœ€åä¼˜åŒ–æ­¥æ•°

                    # æ—¶é—´åœæ­¢æ£€æŸ¥
                    if self.args.time:
                        self.stop = (time.time() - self.train_time_start) > (self.args.time * 3600)
                        if self.stop:  # è¶…è¿‡è®­ç»ƒæ—¶é—´
                            break

                # æ—¥å¿—è®°å½•
                loss_length = self.tloss.shape[0] if len(self.tloss.shape) else 1
                pbar.set_description(
                    ("%11s" * 2 + "%11.4g" * (2 + loss_length))
                    % (
                        f"{epoch + 1}/{self.epochs}",
                        f"{self._get_memory():.3g}G",  # GPU å†…å­˜ä½¿ç”¨æƒ…å†µ
                        *(self.tloss if loss_length > 1 else torch.unsqueeze(self.tloss, 0)),  # æŸå¤±å€¼
                        batch["cls"].shape[0],  # å½“å‰æ‰¹é‡å¤§å°
                        batch["img"].shape[-1],  # å›¾åƒå¤§å°
                    )
                )
                if self.args.plots and ni in self.plot_idx:
                    self.plot_training_samples(batch, ni)  # ç»˜åˆ¶è®­ç»ƒæ ·æœ¬

            # å­¦ä¹ ç‡è®°å½•
            self.lr = {f"lr/pg{ir}": x["lr"] for ir, x in enumerate(self.optimizer.param_groups)}  # è®°å½•å­¦ä¹ ç‡
            final_epoch = epoch + 1 >= self.epochs  # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€åä¸€ä¸ª epoch
            self.ema.update_attr(self.model, include=["yaml", "nc", "args", "names", "stride", "class_weights"])  # æ›´æ–° EMA

            # éªŒè¯
            if self.args.val or final_epoch or self.stopper.possible_stop or self.stop:
                self.metrics, self.fitness = self.validate()  # éªŒè¯æ¨¡å‹
            self.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})  # ä¿å­˜æŒ‡æ ‡
            self.stop |= self.stopper(epoch + 1, self.fitness) or final_epoch  # æ£€æŸ¥æ˜¯å¦åœæ­¢è®­ç»ƒ
            if self.args.time:
                self.stop |= (time.time() - self.train_time_start) > (self.args.time * 3600)  # æ—¶é—´åœæ­¢æ£€æŸ¥

            # ä¿å­˜æ¨¡å‹
            if self.args.save or final_epoch:
                self.save_model()  # ä¿å­˜æ¨¡å‹

            # å­¦ä¹ ç‡è°ƒåº¦
            t = time.time()
            self.epoch_time = t - self.epoch_time  # è®¡ç®— epoch æ—¶é—´
            self.epoch_time_start = t  # æ›´æ–°å¼€å§‹æ—¶é—´
            if self.args.time:
                mean_epoch_time = (t - self.train_time_start) / (epoch - self.start_epoch + 1)  # å¹³å‡ epoch æ—¶é—´
                self.epochs = self.args.epochs = math.ceil(self.args.time * 3600 / mean_epoch_time)  # æ›´æ–°æ€» epoch
                self._setup_scheduler()  # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
                self.scheduler.last_epoch = self.epoch  # ä¸ç§»åŠ¨
                self.stop |= epoch >= self.epochs  # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ€» epoch
            self._clear_memory()  # æ¸…ç†å†…å­˜

            # æå‰åœæ­¢æ£€æŸ¥
            if self.stop:
                break  # ç»“æŸè®­ç»ƒ
            epoch += 1  # æ›´æ–° epoch è®¡æ•°

        # æœ€ç»ˆéªŒè¯
        seconds = time.time() - self.train_time_start
        LOGGER.info(f"\n{epoch - self.start_epoch + 1} ä¸ªå‘¨æœŸå®Œæˆï¼Œè€—æ—¶ {seconds / 3600:.3f} å°æ—¶ã€‚")
        self.final_eval()  # æœ€ç»ˆè¯„ä¼°
        if self.args.plots:
            self.plot_metrics()  # ç»˜åˆ¶æŒ‡æ ‡
        self._clear_memory()  # æ¸…ç†å†…å­˜

    def _get_memory(self):
        """è·å–åŠ é€Ÿå™¨çš„å†…å­˜åˆ©ç”¨ç‡ï¼ˆå•ä½ï¼šGBï¼‰ã€‚"""
        if self.device.type == "mps":
            memory = torch.mps.driver_allocated_memory()  # MPS è®¾å¤‡å†…å­˜
        elif self.device.type == "cpu":
            memory = 0  # CPU å†…å­˜
        else:
            memory = torch.cuda.memory_reserved()  # CUDA è®¾å¤‡å†…å­˜
        return memory / 1e9  # è¿”å›å†…å­˜ä»¥ GB ä¸ºå•ä½

    def _clear_memory(self):
        """åœ¨ä¸åŒå¹³å°ä¸Šæ¸…ç†åŠ é€Ÿå™¨å†…å­˜ã€‚"""
        gc.collect()  # åƒåœ¾å›æ”¶
        if self.device.type == "mps":
            torch.mps.empty_cache()  # æ¸…ç©º MPS ç¼“å­˜
        elif self.device.type == "cpu":
            return  # CPU ä¸éœ€è¦æ“ä½œ
        else:
            torch.cuda.empty_cache()  # æ¸…ç©º CUDA ç¼“å­˜

    def read_results_csv(self):
        """è¯»å– results.csv å¹¶è¿”å›å­—å…¸æ ¼å¼çš„æ•°æ®ã€‚"""
        import pandas as pd  # å»¶è¿Ÿå¯¼å…¥ä»¥åŠ å¿«é€Ÿåº¦

        return pd.read_csv(self.csv).to_dict(orient="list")  # å°† CSV è½¬ä¸ºå­—å…¸

    def save_model(self):
        """ä¿å­˜æ¨¡å‹è®­ç»ƒæ£€æŸ¥ç‚¹åŠé™„åŠ å…ƒæ•°æ®ã€‚"""
        import io

        # å°†æ£€æŸ¥ç‚¹åºåˆ—åŒ–åˆ°å­—èŠ‚ç¼“å­˜ä¸­ï¼ˆæ¯”é‡å¤è°ƒç”¨ torch.save() æ›´å¿«ï¼‰
        buffer = io.BytesIO()
        torch.save(
            {
                "epoch": self.epoch,  # å½“å‰ epoch
                "best_fitness": self.best_fitness,  # æœ€ä½³é€‚åº”åº¦
                "model": None,  # ç”± EMA æ´¾ç”Ÿçš„æ£€æŸ¥ç‚¹
                "ema": deepcopy(self.ema.ema).half(),  # EMA æ¨¡å‹
                "updates": self.ema.updates,  # EMA æ›´æ–°æ¬¡æ•°
                "optimizer": convert_optimizer_state_dict_to_fp16(deepcopy(self.optimizer.state_dict())),  # ä¼˜åŒ–å™¨çŠ¶æ€
                "train_args": vars(self.args),  # ä¿å­˜å‚æ•°å­—å…¸
                "train_metrics": {**self.metrics, **{"fitness": self.fitness}},  # è®­ç»ƒæŒ‡æ ‡
                "train_results": self.read_results_csv(),  # è¯»å–è®­ç»ƒç»“æœ
                "date": datetime.now().isoformat(),  # å½“å‰æ—¥æœŸæ—¶é—´
            },
            buffer,
        )
        serialized_ckpt = buffer.getvalue()  # è·å–åºåˆ—åŒ–å†…å®¹

        # ä¿å­˜æ£€æŸ¥ç‚¹
        self.last.write_bytes(serialized_ckpt)  # ä¿å­˜ last.pt
        if self.best_fitness == self.fitness:
            self.best.write_bytes(serialized_ckpt)  # ä¿å­˜ best.pt
        if (self.save_period > 0) and (self.epoch % self.save_period == 0):
            (self.wdir / f"epoch{self.epoch}.pt").write_bytes(serialized_ckpt)  # ä¿å­˜å½“å‰ epoch

    def get_dataset(self):
        """æ£€æŸ¥å¹¶è·å–æ•°æ®é›†ã€‚"""
        data = check_det_dataset(self.args.data)  # æ£€æŸ¥æ•°æ®é›†
        if "yaml_file" in data:
            self.args.data = data["yaml_file"]  # éªŒè¯ 'yolo train data=url.zip' ä½¿ç”¨
        self.data = data
        return data["train"], data.get("val") or data.get("test")  # è¿”å›è®­ç»ƒé›†å’ŒéªŒè¯é›†

    def setup_model(self):
        """åŠ è½½/åˆ›å»º/ä¸‹è½½æ¨¡å‹ä»¥ç”¨äºä»»ä½•ä»»åŠ¡ã€‚"""
        if isinstance(self.model, torch.nn.Module):  # å¦‚æœæ¨¡å‹å·²åŠ è½½ï¼Œåˆ™æ— éœ€è®¾ç½®
            return

        cfg, weights = self.model, None
        ckpt = None
        if str(self.model).endswith(".pt"):
            weights, ckpt = attempt_load_one_weight(self.model)  # å°è¯•åŠ è½½æƒé‡
            cfg = weights.yaml  # è·å–é…ç½®
        elif isinstance(self.args.pretrained, (str, Path)):
            weights, _ = attempt_load_one_weight(self.args.pretrained)  # å°è¯•åŠ è½½é¢„è®­ç»ƒæƒé‡
        self.model = self.get_model(cfg=cfg, weights=weights, verbose=True)  # åˆ›å»ºæ¨¡å‹
        return ckpt

    def optimizer_step(self):
        """æ‰§è¡Œä¸€æ¬¡è®­ç»ƒä¼˜åŒ–å™¨æ­¥éª¤ï¼ŒåŒ…å«æ¢¯åº¦è£å‰ªå’Œ EMA æ›´æ–°ã€‚"""
        self.scaler.unscale_(self.optimizer)  # å–æ¶ˆç¼©æ”¾æ¢¯åº¦
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)  # è£å‰ªæ¢¯åº¦
        self.scaler.step(self.optimizer)  # æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤
        self.scaler.update()  # æ›´æ–°ç¼©æ”¾å™¨
        self.optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
        if self.ema:
            self.ema.update(self.model)  # æ›´æ–° EMA

    def preprocess_batch(self, batch):
        """é¢„å¤„ç†ä¸€æ‰¹å›¾åƒï¼Œè¿›è¡Œç¼©æ”¾å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚"""
        batch["img"] = batch["img"].to(self.device, non_blocking=True).float() / 255  # è½¬æ¢å›¾åƒ
        if self.args.multi_scale:
            imgs = batch["img"]
            sz = (
                    random.randrange(int(self.args.imgsz * 0.5), int(self.args.imgsz * 1.5 + self.stride))
                    // self.stride
                    * self.stride
            )  # éšæœºå¤§å°
            sf = sz / max(imgs.shape[2:])  # ç¼©æ”¾å› å­
            if sf != 1:
                ns = [
                    math.ceil(x * sf / self.stride) * self.stride for x in imgs.shape[2:]
                ]  # æ–°å½¢çŠ¶ï¼ˆæ‹‰ä¼¸åˆ°ç½‘æ ¼å€æ•°ï¼‰
                imgs = nn.functional.interpolate(imgs, size=ns, mode="bilinear", align_corners=False)  # é‡æ–°è°ƒæ•´å›¾åƒå¤§å°
            batch["img"] = imgs  # æ›´æ–°æ‰¹æ¬¡å›¾åƒ
        return batch

    def validate(self):
        """éªŒè¯æ¨¡å‹å¹¶è¿”å›æŒ‡æ ‡ã€‚"""
        metrics = self.validator(self)  # ä½¿ç”¨éªŒè¯å™¨è¿›è¡ŒéªŒè¯
        fitness = metrics.pop("fitness", -self.loss.detach().cpu().numpy())  # ä½¿ç”¨æŸå¤±ä½œä¸ºé€‚åº”åº¦
        if not self.best_fitness or self.best_fitness < fitness:
            self.best_fitness = fitness  # æ›´æ–°æœ€ä½³é€‚åº”åº¦
        return metrics, fitness  # è¿”å›æŒ‡æ ‡å’Œé€‚åº”åº¦

    def get_model(self, cfg=None, weights=None, verbose=True):
        """åˆ›å»ºå¹¶è¿”å›æ£€æµ‹æ¨¡å‹ã€‚"""
        model = DetectionModel(cfg, nc=self.data["nc"], verbose=verbose)  # åˆ›å»ºæ£€æµ‹æ¨¡å‹
        if weights:
            model.load(weights)  # åŠ è½½æƒé‡
        return model

    def get_validator(self):
        """è¿”å›ç”¨äº YOLO æ¨¡å‹éªŒè¯çš„ DetectionValidatorã€‚"""
        self.loss_names = "box_loss", "cls_loss", "dfl_loss"  # æŸå¤±åç§°
        return DetectionValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))  # åˆ›å»ºéªŒè¯å™¨

    def get_dataloader(self, dataset_path, batch_size=16, mode="train"):
        """æ„å»ºå¹¶è¿”å›æ•°æ®åŠ è½½å™¨ã€‚"""
        assert mode in {"train", "val"}, f"æ¨¡å¼å¿…é¡»ä¸º 'train' æˆ– 'val'ï¼Œè€Œä¸æ˜¯ {mode}ã€‚"
        dataset = self.build_dataset(dataset_path, mode, batch_size)  # æ„å»ºæ•°æ®é›†
        shuffle = mode == "train"  # è®­ç»ƒæ—¶æ‰“ä¹±æ•°æ®
        if getattr(dataset, "rect", False) and shuffle:
            LOGGER.warning("è­¦å‘Š âš ï¸ 'rect=True' ä¸ DataLoader æ‰“ä¹±ä¸å…¼å®¹ï¼Œè®¾ç½® shuffle=False")
            shuffle = False  # å¦‚æœçŸ©å½¢æ¨¡å¼ï¼Œå…³é—­æ‰“ä¹±
        workers = self.args.workers if mode == "train" else self.args.workers * 2  # è®¾ç½®å·¥ä½œçº¿ç¨‹æ•°
        return build_dataloader(dataset, batch_size, workers, shuffle)  # è¿”å›æ•°æ®åŠ è½½å™¨

    def build_dataset(self, img_path, mode="train", batch=None):
        """æ„å»ºå¹¶è¿”å› YOLO æ•°æ®é›†ã€‚"""
        gs = max(int(de_parallel(self.model).stride.max() if self.model else 0), 32)  # ç½‘æ ¼å¤§å°
        return build_yolo_dataset(self.args, img_path, batch, self.data, mode=mode, rect=mode == "val", stride=gs)  # è¿”å›æ•°æ®é›†

    def set_model_attributes(self):
        """è®¾ç½®æ¨¡å‹å±æ€§ï¼Œå¦‚ç±»åˆ«æ•°é‡å’Œåç§°ã€‚"""
        self.model.nc = self.data["nc"]  # é™„åŠ ç±»åˆ«æ•°é‡åˆ°æ¨¡å‹
        self.model.names = self.data["names"]  # é™„åŠ ç±»åˆ«åç§°åˆ°æ¨¡å‹
        self.model.args = self.args  # é™„åŠ è¶…å‚æ•°åˆ°æ¨¡å‹

    def label_loss_items(self, loss_items=None, prefix="train"):
        """
        è¿”å›å¸¦æ ‡ç­¾çš„æŸå¤±å­—å…¸ã€‚

        å¯¹äºåˆ†ç±»ä¸éœ€è¦ï¼Œä½†å¯¹äºåˆ†å‰²å’Œæ£€æµ‹æ˜¯å¿…éœ€çš„ã€‚
        """
        keys = [f"{prefix}/{x}" for x in self.loss_names]  # åˆ›å»ºå¸¦å‰ç¼€çš„æŸå¤±åç§°
        if loss_items is not None:
            loss_items = [round(float(x), 5) for x in loss_items]  # å°†å¼ é‡è½¬æ¢ä¸º 5 ä½å°æ•°
            return dict(zip(keys, loss_items))  # è¿”å›å­—å…¸
        else:
            return keys  # è¿”å›æŸå¤±åç§°

    def progress_string(self):
        """è¿”å›æ ¼å¼åŒ–çš„è®­ç»ƒè¿›åº¦å­—ç¬¦ä¸²ã€‚"""
        return ("\n" + "%11s" * (4 + len(self.loss_names))) % (
            "Epoch",  # å½“å‰å‘¨æœŸ
            "GPU_mem",  # GPU å†…å­˜
            *self.loss_names,  # æŸå¤±åç§°
            "Instances",  # å®ä¾‹æ•°é‡
            "Size",  # å›¾åƒå¤§å°
        )

    def plot_training_samples(self, batch, ni):
        """ç»˜åˆ¶å¸¦æœ‰æ³¨é‡Šçš„è®­ç»ƒæ ·æœ¬ã€‚"""
        plot_images(
            images=batch["img"],  # å›¾åƒ
            batch_idx=batch["batch_idx"],  # æ‰¹æ¬¡ç´¢å¼•
            cls=batch["cls"].squeeze(-1),  # ç±»åˆ«
            bboxes=batch["bboxes"],  # è¾¹ç•Œæ¡†
            paths=batch["im_file"],  # æ–‡ä»¶è·¯å¾„
            fname=self.save_dir / f"train_batch{ni}.jpg",  # ä¿å­˜æ–‡ä»¶å
            on_plot=self.on_plot,  # ç»˜å›¾å›è°ƒ
        )

    def plot_metrics(self):
        """ç»˜åˆ¶æ¥è‡ª CSV æ–‡ä»¶çš„æŒ‡æ ‡ã€‚"""
        plot_results(file=self.csv, on_plot=self.on_plot)  # ä¿å­˜ç»“æœå›¾

    def plot_training_labels(self):
        """åˆ›å»º YOLO æ¨¡å‹çš„æ ‡è®°è®­ç»ƒå›¾ã€‚"""
        boxes = np.concatenate([lb["bboxes"] for lb in self.train_loader.dataset.labels], 0)  # åˆå¹¶æ‰€æœ‰è¾¹ç•Œæ¡†
        cls = np.concatenate([lb["cls"] for lb in self.train_loader.dataset.labels], 0)  # åˆå¹¶æ‰€æœ‰ç±»åˆ«
        plot_labels(boxes, cls.squeeze(), names=self.data["names"], save_dir=self.save_dir, on_plot=self.on_plot)  # ç»˜åˆ¶æ ‡ç­¾

    def save_metrics(self, metrics):
        """å°†è®­ç»ƒæŒ‡æ ‡ä¿å­˜åˆ° CSV æ–‡ä»¶ã€‚"""
        keys, vals = list(metrics.keys()), list(metrics.values())
        n = len(metrics) + 2  # åˆ—æ•°
        s = "" if self.csv.exists() else (("%s," * n % tuple(["epoch", "time"] + keys)).rstrip(",") + "\n")  # å¤´éƒ¨
        t = time.time() - self.train_time_start  # è®¡ç®—ç»è¿‡æ—¶é—´
        with open(self.csv, "a") as f:
            f.write(s + ("%.6g," * n % tuple([self.epoch + 1, t] + vals)).rstrip(",") + "\n")  # å†™å…¥æ•°æ®

    def on_plot(self, name, data=None):
        """æ³¨å†Œç»˜å›¾ï¼ˆä¾‹å¦‚ï¼Œä¾›å›è°ƒä½¿ç”¨ï¼‰ã€‚"""
        path = Path(name)  # å°†åç§°è½¬æ¢ä¸ºè·¯å¾„
        self.plots[path] = {"data": data, "timestamp": time.time()}  # å­˜å‚¨ç»˜å›¾æ•°æ®å’Œæ—¶é—´æˆ³

    def final_eval(self):
        """æ‰§è¡Œæœ€ç»ˆè¯„ä¼°å’ŒéªŒè¯ YOLO æ¨¡å‹ã€‚"""
        ckpt = {}
        for f in self.last, self.best:
            if f.exists():
                if f is self.last:
                    ckpt = strip_optimizer(f)  # ä» last.pt ä¸­ç§»é™¤ä¼˜åŒ–å™¨
                elif f is self.best:
                    k = "train_results"  # ä» last.pt æ›´æ–° best.pt çš„è®­ç»ƒæŒ‡æ ‡
                    strip_optimizer(f, updates={k: ckpt[k]} if k in ckpt else None)
                    LOGGER.info(f"\néªŒè¯ {f}...")
                    self.validator.args.plots = self.args.plots  # æ›´æ–°ç»˜å›¾å‚æ•°
                    self.metrics = self.validator(model=f)  # éªŒè¯æ¨¡å‹
                    self.metrics.pop("fitness", None)  # ç§»é™¤é€‚åº”åº¦

    def check_resume(self, overrides):
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¢å¤æ£€æŸ¥ç‚¹ï¼Œå¹¶ç›¸åº”æ›´æ–°å‚æ•°ã€‚"""
        resume = self.args.resume  # æ¢å¤æ ‡å¿—
        if resume:
            try:
                exists = isinstance(resume, (str, Path)) and Path(resume).exists()  # æ£€æŸ¥æ¢å¤è·¯å¾„æ˜¯å¦å­˜åœ¨
                last = Path(resume if exists else get_latest_run())  # è·å–æœ€æ–°æ£€æŸ¥ç‚¹

                # æ£€æŸ¥æ¢å¤æ•°æ® YAML æ˜¯å¦å­˜åœ¨ï¼Œå¦åˆ™å¼ºåˆ¶é‡æ–°ä¸‹è½½æ•°æ®é›†
                ckpt_args = attempt_load_weights(last).args  # è·å–æ£€æŸ¥ç‚¹å‚æ•°
                if not Path(ckpt_args["data"]).exists():
                    ckpt_args["data"] = self.args.data  # æ›´æ–°æ•°æ®è·¯å¾„

                resume = True
                self.args = get_cfg(ckpt_args)  # é‡æ–°åŠ è½½é…ç½®
                self.args.model = self.args.resume = str(last)  # é‡æ–°è®¾ç½®æ¨¡å‹
                for k in (
                    "imgsz",
                    "batch",
                    "device",
                    "close_mosaic",
                ):  # å…è®¸å‚æ•°æ›´æ–°
                    if k in overrides:
                        setattr(self.args, k, overrides[k])  # æ›´æ–°å‚æ•°

            except Exception as e:
                raise FileNotFoundError("æ¢å¤æ£€æŸ¥ç‚¹æœªæ‰¾åˆ°ã€‚") from e
        self.resume = resume  # è®¾ç½®æ¢å¤æ ‡å¿—

    def resume_training(self, ckpt):
        """ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚"""
        if ckpt is None or not self.resume:
            return
        best_fitness = 0.0  # æœ€ä½³é€‚åº”åº¦åˆå§‹åŒ–
        start_epoch = ckpt.get("epoch", -1) + 1  # æ¢å¤çš„èµ·å§‹ epoch
        if ckpt.get("optimizer", None) is not None:
            self.optimizer.load_state_dict(ckpt["optimizer"])  # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
            best_fitness = ckpt["best_fitness"]  # æ¢å¤æœ€ä½³é€‚åº”åº¦
        if self.ema and ckpt.get("ema"):
            self.ema.ema.load_state_dict(ckpt["ema"].float().state_dict())  # æ¢å¤ EMA
            self.ema.updates = ckpt["updates"]  # æ¢å¤æ›´æ–°æ¬¡æ•°
        assert start_epoch > 0, (
            f"{self.args.model} è®­ç»ƒåˆ° {self.epochs} ä¸ªå‘¨æœŸå·²å®Œæˆï¼Œæ— éœ€æ¢å¤ã€‚\n"
            f"å¼€å§‹æ–°è®­ç»ƒï¼Œè€Œä¸æ˜¯æ¢å¤ï¼Œi.e. 'yolo train model={self.args.model}'"
        )
        LOGGER.info(f"æ¢å¤è®­ç»ƒ {self.args.model} ä»ç¬¬ {start_epoch + 1} ä¸ªå‘¨æœŸåˆ°æ€»å…± {self.epochs} ä¸ªå‘¨æœŸ")
        if self.epochs < start_epoch:
            LOGGER.info(
                f"{self.model} å·²è®­ç»ƒ {ckpt['epoch']} ä¸ªå‘¨æœŸã€‚å¾®è°ƒ {self.epochs} ä¸ªå‘¨æœŸã€‚"
            )
            self.epochs += ckpt["epoch"]  # å¾®è°ƒé¢å¤–çš„å‘¨æœŸ
        self.best_fitness = best_fitness  # æ›´æ–°æœ€ä½³é€‚åº”åº¦
        self.start_epoch = start_epoch  # æ›´æ–°èµ·å§‹ epoch
        if start_epoch > (self.epochs - self.args.close_mosaic):
            self._close_dataloader_mosaic()  # å…³é—­é©¬èµ›å…‹å¢å¼º

    def _close_dataloader_mosaic(self):
        """æ›´æ–°æ•°æ®åŠ è½½å™¨ä»¥åœæ­¢ä½¿ç”¨é©¬èµ›å…‹å¢å¼ºã€‚"""
        if hasattr(self.train_loader.dataset, "mosaic"):
            self.train_loader.dataset.mosaic = False  # å…³é—­é©¬èµ›å…‹
        if hasattr(self.train_loader.dataset, "close_mosaic"):
            LOGGER.info("å…³é—­æ•°æ®åŠ è½½å™¨é©¬èµ›å…‹")
            self.train_loader.dataset.close_mosaic(hyp=copy(self.args))  # å…³é—­é©¬èµ›å…‹å¢å¼º

    def build_optimizer(self, model, name="auto", lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5):
        """
        æ„å»ºä¼˜åŒ–å™¨ã€‚

        å‚æ•°ï¼š
            model (torch.nn.Module): è¦ä¸ºå…¶æ„å»ºä¼˜åŒ–å™¨çš„æ¨¡å‹ã€‚
            name (str, optional): ä½¿ç”¨çš„ä¼˜åŒ–å™¨åç§°ã€‚å¦‚æœä¸º 'auto'ï¼Œåˆ™æ ¹æ®è¿­ä»£æ¬¡æ•°é€‰æ‹©ä¼˜åŒ–å™¨ã€‚é»˜è®¤å€¼ï¼š'auto'ã€‚
            lr (float, optional): ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ã€‚é»˜è®¤å€¼ï¼š0.001ã€‚
            momentum (float, optional): ä¼˜åŒ–å™¨çš„åŠ¨é‡å› å­ã€‚é»˜è®¤å€¼ï¼š0.9ã€‚
            decay (float, optional): ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡ã€‚é»˜è®¤å€¼ï¼š1e-5ã€‚
            iterations (float, optional): è¿­ä»£æ¬¡æ•°ï¼Œå†³å®šä¼˜åŒ–å™¨ç±»å‹ï¼ˆå¦‚æœåç§°ä¸º 'auto'ï¼‰ã€‚é»˜è®¤å€¼ï¼š1e5ã€‚
        """
        g = [], [], []  # ä¼˜åŒ–å™¨å‚æ•°ç»„
        bn = tuple(v for k, v in nn.__dict__.items() if "Norm" in k)  # å½’ä¸€åŒ–å±‚ï¼Œä¾‹å¦‚ BatchNorm2d()
        if name == "auto":
            LOGGER.info(
                f"{colorstr('optimizer:')} 'optimizer=auto' è¢«æ‰¾åˆ°ï¼Œ"
                f"å¿½ç•¥ 'lr0={self.args.lr0}' å’Œ 'momentum={self.args.momentum}'ï¼Œ"
                f"è‡ªåŠ¨ç¡®å®šæœ€ä½³ 'optimizer'ã€'lr0' å’Œ 'momentum'... "
            )
            nc = getattr(model, "nc", 10)  # ç±»åˆ«æ•°é‡
            lr_fit = round(0.002 * 5 / (4 + nc), 6)  # lr0 é€‚é…æ–¹ç¨‹
            name, lr, momentum = ("SGD", 0.01, 0.9) if iterations > 10000 else ("AdamW", lr_fit, 0.9)
            self.args.warmup_bias_lr = 0.0  # Adam çš„å­¦ä¹ ç‡ä¸é«˜äº 0.01

        for module_name, module in model.named_modules():
            for param_name, param in module.named_parameters(recurse=False):
                fullname =f"{module_name}.{param_name}" if module_name else param_name  # å®Œæ•´å‚æ•°åç§°
                if "bias" in fullname:  # åç½®å‚æ•°ï¼ˆä¸è¡°å‡ï¼‰
                    g[2].append(param)  # æ·»åŠ åˆ°åç½®å‚æ•°ç»„
                elif isinstance(module, bn):  # å½’ä¸€åŒ–å±‚ï¼ˆä¸è¡°å‡ï¼‰
                    g[1].append(param)  # æ·»åŠ åˆ°å½’ä¸€åŒ–å‚æ•°ç»„
                else:  # æƒé‡å‚æ•°ï¼ˆè¡°å‡ï¼‰
                    g[0].append(param)  # æ·»åŠ åˆ°æƒé‡å‚æ•°ç»„

        # å¯ç”¨çš„ä¼˜åŒ–å™¨
        optimizers = {"Adam", "Adamax", "AdamW", "NAdam", "RAdam", "RMSProp", "SGD", "auto"}
        name = {x.lower(): x for x in optimizers}.get(name.lower())  # å°†åç§°è½¬ä¸ºå°å†™
        if name in {"Adam", "Adamax", "AdamW", "NAdam", "RAdam"}:
            optimizer = getattr(optim, name, optim.Adam)(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)  # åˆ›å»º Adam ç±»ä¼˜åŒ–å™¨
        elif name == "RMSProp":
            optimizer = optim.RMSprop(g[2], lr=lr, momentum=momentum)  # åˆ›å»º RMSProp ä¼˜åŒ–å™¨
        elif name == "SGD":
            optimizer = optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)  # åˆ›å»º SGD ä¼˜åŒ–å™¨
        else:
            raise NotImplementedError(f"ä¼˜åŒ–å™¨ '{name}' æœªæ‰¾åˆ°ã€‚")  # æŠ›å‡ºæœªå®ç°é”™è¯¯

        optimizer.add_param_group({"params": g[0], "weight_decay": decay})  # æ·»åŠ  g0ï¼ˆå¸¦æƒé‡è¡°å‡çš„æƒé‡å‚æ•°ï¼‰
        optimizer.add_param_group({"params": g[1], "weight_decay": 0.0})  # æ·»åŠ  g1ï¼ˆä¸å¸¦è¡°å‡çš„å½’ä¸€åŒ–å±‚æƒé‡ï¼‰
        LOGGER.info(
            f"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}, momentum={momentum}) å¸¦å‚æ•°ç»„ "
            f'{len(g[1])} æƒé‡(è¡°å‡=0.0)ï¼Œ{len(g[0])} æƒé‡(è¡°å‡={decay})ï¼Œ{len(g[2])} åç½®(è¡°å‡=0.0)'
        )
        return optimizer  # è¿”å›ä¼˜åŒ–å™¨å®ä¾‹