# Ultralytics YOLOLite ğŸš€, AGPL-3.0 license

from itertools import repeat
import torch
from yololite.utils.ops import resample_segments

from .augment import (
    Compose,
    Format,
    Instances,
    LetterBox,
    v8_transforms,
)
from .utils import (
    get_hash,
    img2label_paths,
    load_dataset_cache_file,
    save_dataset_cache_file,
    verify_image_label,
)

DATASET_CACHE_VERSION = "1.0.3"
import glob
import math
import os
import random
from copy import deepcopy
from multiprocessing.pool import ThreadPool
from pathlib import Path
from typing import Optional
import cv2
import numpy as np
import psutil
from torch.utils.data import Dataset
from yololite.data.utils import FORMATS_HELP_MSG, HELP_URL, IMG_FORMATS
from yololite.utils import DEFAULT_CFG, LOGGER, NUM_THREADS, TQDM


class YOLODataset(Dataset):
    """
    ç”¨äºåŠ è½½å’Œå¤„ç†å›¾åƒæ•°æ®çš„åŸºç¡€æ•°æ®é›†ç±»ã€‚

    å‚æ•°ï¼š
        img_path (str): åŒ…å«å›¾åƒçš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
        imgsz (int, optional): å›¾åƒå¤§å°ã€‚é»˜è®¤ä¸º 640ã€‚
        cache (bool, optional): åœ¨è®­ç»ƒæœŸé—´å°†å›¾åƒç¼“å­˜åˆ° RAM æˆ–ç£ç›˜ã€‚é»˜è®¤ä¸º Falseã€‚
        augment (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™åº”ç”¨æ•°æ®å¢å¼ºã€‚é»˜è®¤ä¸º Trueã€‚
        hyp (dict, optional): åº”ç”¨æ•°æ®å¢å¼ºçš„è¶…å‚æ•°ã€‚é»˜è®¤ä¸º Noneã€‚
        prefix (str, optional): æ—¥å¿—æ¶ˆæ¯ä¸­æ‰“å°çš„å‰ç¼€ã€‚é»˜è®¤ä¸º ''ã€‚
        rect (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™ä½¿ç”¨çŸ©å½¢è®­ç»ƒã€‚é»˜è®¤ä¸º Falseã€‚
        batch_size (int, optional): æ‰¹é‡å¤§å°ã€‚é»˜è®¤ä¸º Noneã€‚
        stride (int, optional): æ­¥å¹…ã€‚é»˜è®¤ä¸º 32ã€‚
        pad (float, optional): å¡«å……ã€‚é»˜è®¤ä¸º 0.0ã€‚
        single_cls (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™ä½¿ç”¨å•ç±»è®­ç»ƒã€‚é»˜è®¤ä¸º Falseã€‚
        classes (list): åŒ…å«çš„ç±»åˆ«åˆ—è¡¨ã€‚é»˜è®¤ä¸º Noneã€‚

    å±æ€§ï¼š
        im_files (list): å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
        labels (list): æ ‡ç­¾æ•°æ®å­—å…¸åˆ—è¡¨ã€‚
        ni (int): æ•°æ®é›†ä¸­å›¾åƒçš„æ•°é‡ã€‚
        ims (list): åŠ è½½çš„å›¾åƒåˆ—è¡¨ã€‚
        npy_files (list): Numpy æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
        transforms (callable): å›¾åƒè½¬æ¢å‡½æ•°ã€‚
    """

    def __init__(
            self,
            img_path,
            imgsz=640,
            cache=False,
            augment=True,
            hyp=DEFAULT_CFG,
            prefix="",
            rect=False,
            batch_size=16,
            stride=32,
            pad=0.5,
            single_cls=False,
            classes=None,
            data=None
    ):
        super().__init__()
        self.img_path = img_path  # å›¾åƒè·¯å¾„
        self.imgsz = imgsz  # å›¾åƒå¤§å°
        self.augment = augment  # æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
        self.single_cls = single_cls  # æ˜¯å¦ä½¿ç”¨å•ç±»è®­ç»ƒ
        self.prefix = prefix  # æ—¥å¿—å‰ç¼€
        self.im_files = self.get_img_files(self.img_path)  # è·å–å›¾åƒæ–‡ä»¶è·¯å¾„
        self.labels = self.get_labels()  # è·å–æ ‡ç­¾ä¿¡æ¯
        self.update_labels(include_class=classes)  # æ›´æ–°æ ‡ç­¾ä»¥åŒ…å«æŒ‡å®šç±»åˆ«
        self.ni = len(self.labels)  # æ•°æ®é›†ä¸­å›¾åƒçš„æ•°é‡
        self.rect = rect  # æ˜¯å¦ä½¿ç”¨çŸ©å½¢è®­ç»ƒ
        self.batch_size = batch_size  # æ‰¹é‡å¤§å°
        self.stride = stride  # æ­¥å¹…
        self.pad = pad  # å¡«å……
        self.data = data  # å…¶ä»–æ•°æ®ï¼ˆå¯é€‰ï¼‰

        if self.rect:
            assert self.batch_size is not None  # ç¡®ä¿æ‰¹é‡å¤§å°å·²å®šä¹‰
            self.set_rectangle()  # è®¾ç½®çŸ©å½¢è®­ç»ƒå‚æ•°

        # ç¼“å­˜å›¾åƒé…ç½®
        self.buffer = []  # ç”¨äºå­˜å‚¨å›¾åƒçš„ç¼“å†²åŒº
        self.max_buffer_length = min((self.ni, self.batch_size * 8, 1000)) if self.augment else 0  # æœ€å¤§ç¼“å†²åŒºé•¿åº¦

        # ç¼“å­˜å›¾åƒï¼ˆé€‰é¡¹ä¸ºï¼šcache = True, False, None, "ram", "disk"ï¼‰
        self.ims, self.im_hw0, self.im_hw = [None] * self.ni, [None] * self.ni, [None] * self.ni  # å›¾åƒåŠå…¶å°ºå¯¸ç¼“å­˜
        self.npy_files = [Path(f).with_suffix(".npy") for f in self.im_files]  # Numpy æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        self.cache = cache.lower() if isinstance(cache, str) else "ram" if cache is True else None  # ç¡®å®šç¼“å­˜æ–¹å¼

        # æ£€æŸ¥å¹¶åŠ è½½å›¾åƒç¼“å­˜
        if self.cache == "ram" and self.check_cache_ram():
            if hyp.deterministic:
                LOGGER.warning(
                    "è­¦å‘Š âš ï¸ cache='ram' å¯èƒ½ä¼šäº§ç”Ÿéç¡®å®šæ€§çš„è®­ç»ƒç»“æœã€‚"
                    "å¦‚æœæ‚¨çš„ç£ç›˜ç©ºé—´å…è®¸ï¼Œè¯·è€ƒè™‘ä½¿ç”¨ cache='disk' ä½œä¸ºç¡®å®šæ€§æ›¿ä»£æ–¹æ¡ˆã€‚"
                )
            self.cache_images()  # ç¼“å­˜å›¾åƒåˆ° RAM
        elif self.cache == "disk" and self.check_cache_disk():
            self.cache_images()  # ç¼“å­˜å›¾åƒåˆ°ç£ç›˜

        # è®¾ç½®å›¾åƒè½¬æ¢
        self.transforms = self.build_transforms(hyp=hyp)  # æ„å»ºå›¾åƒè½¬æ¢å‡½æ•°

    def get_img_files(self, img_path):
        try:
            f = []  # å›¾åƒæ–‡ä»¶åˆ—è¡¨
            for p in img_path if isinstance(img_path, list) else [img_path]:
                p = Path(p)  # å¤„ç†è·¯å¾„
                if p.is_dir():  # å¦‚æœæ˜¯æ–‡ä»¶å¤¹
                    f += glob.glob(str(p / "**" / "*.*"), recursive=True)  # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
                elif p.is_file():  # å¦‚æœæ˜¯æ–‡ä»¶
                    with open(p) as t:
                        t = t.read().strip().splitlines()  # è¯»å–æ–‡ä»¶å†…å®¹
                        parent = str(p.parent) + os.sep  # è·å–çˆ¶ç›®å½•
                        f += [x.replace("./", parent) if x.startswith("./") else x for x in t]  # è½¬æ¢ä¸ºå…¨å±€è·¯å¾„
                else:
                    raise FileNotFoundError(f"{self.prefix}{p} ä¸å­˜åœ¨")
            im_files = sorted(x.replace("/", os.sep) for x in f if x.split(".")[-1].lower() in IMG_FORMATS)  # è¿‡æ»¤æœ‰æ•ˆçš„å›¾åƒæ ¼å¼
            assert im_files, f"åœ¨ {img_path} ä¸­æœªæ‰¾åˆ°ä»»ä½•å›¾åƒã€‚ {FORMATS_HELP_MSG}"  # ç¡®ä¿è‡³å°‘æ‰¾åˆ°ä¸€ä¸ªå›¾åƒæ–‡ä»¶
        except Exception as e:
            raise FileNotFoundError(f"ä» {img_path} åŠ è½½æ•°æ®æ—¶å‡ºé”™") from e
        return im_files

    def update_labels(self, include_class: Optional[list]):
        include_class_array = np.array(include_class).reshape(1, -1)  # è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼
        for i in range(len(self.labels)):
            if include_class is not None:
                cls = self.labels[i]["cls"]  # ç±»åˆ«
                bboxes = self.labels[i]["bboxes"]  # è¾¹ç•Œæ¡†
                segments = self.labels[i]["segments"]  # åˆ†æ®µä¿¡æ¯
                keypoints = self.labels[i]["keypoints"]  # å…³é”®ç‚¹ä¿¡æ¯
                j = (cls == include_class_array).any(1)  # æ£€æŸ¥ç±»åˆ«æ˜¯å¦åœ¨æŒ‡å®šç±»åˆ«ä¸­
                self.labels[i]["cls"] = cls[j]  # æ›´æ–°ç±»åˆ«
                self.labels[i]["bboxes"] = bboxes[j]  # æ›´æ–°è¾¹ç•Œæ¡†
                if segments:
                    self.labels[i]["segments"] = [segments[si] for si, idx in enumerate(j) if idx]  # æ›´æ–°åˆ†æ®µä¿¡æ¯
                if keypoints is not None:
                    self.labels[i]["keypoints"] = keypoints[j]  # æ›´æ–°å…³é”®ç‚¹ä¿¡æ¯
            if self.single_cls:  # å¦‚æœä½¿ç”¨å•ç±»è®­ç»ƒ
                self.labels[i]["cls"][:, 0] = 0  # å°†æ‰€æœ‰ç±»åˆ«è®¾ç½®ä¸º 0

    def load_image(self, i, rect_mode=True):
        """ä»æ•°æ®é›†ä¸­åŠ è½½ç¬¬ i å¼ å›¾åƒï¼Œè¿”å› (im, resized hw)ã€‚"""
        im, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]  # è·å–å½“å‰å›¾åƒåŠå…¶æ–‡ä»¶å
        if im is None:  # å¦‚æœå›¾åƒæœªç¼“å­˜
            if fn.exists():  # å¦‚æœå­˜åœ¨ npy æ–‡ä»¶
                try:
                    im = np.load(fn)  # åŠ è½½ npy æ–‡ä»¶
                except Exception as e:
                    LOGGER.warning(f"{self.prefix}è­¦å‘Š âš ï¸ åˆ é™¤æŸåçš„ *.npy å›¾åƒæ–‡ä»¶ {fn}ï¼ŒåŸå› ï¼š{e}")
                    Path(fn).unlink(missing_ok=True)  # åˆ é™¤æŸåçš„æ–‡ä»¶
                    im = cv2.imread(f)  # è¯»å–å›¾åƒ
            else:  # å¦‚æœæ˜¯æ™®é€šå›¾åƒæ–‡ä»¶
                im = cv2.imread(f)  # è¯»å–å›¾åƒ
            if im is None:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°å›¾åƒ {f}")

            h0, w0 = im.shape[:2]  # åŸå§‹å›¾åƒé«˜å®½
            if rect_mode:  # å¦‚æœæ˜¯çŸ©å½¢æ¨¡å¼
                r = self.imgsz / max(h0, w0)  # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
                if r != 1:  # å¦‚æœé«˜å®½ä¸ç›¸ç­‰
                    w, h = (min(math.ceil(w0 * r), self.imgsz), min(math.ceil(h0 * r), self.imgsz))  # è®¡ç®—æ–°çš„é«˜å®½
                    im = cv2.resize(im, (w, h), interpolation=cv2.INTER_LINEAR)  # è°ƒæ•´å›¾åƒå¤§å°
            elif not (h0 == w0 == self.imgsz):  # å¦‚æœä¸æ˜¯æ­£æ–¹å½¢ä¸”ä¸æ˜¯ç›®æ ‡å¤§å°
                im = cv2.resize(im, (self.imgsz, self.imgsz), interpolation=cv2.INTER_LINEAR)  # å¼ºåˆ¶è°ƒæ•´ä¸ºæ­£æ–¹å½¢å¤§å°

            # å¦‚æœåœ¨è®­ç»ƒæ—¶ä½¿ç”¨æ•°æ®å¢å¼ºï¼Œåˆ™æ·»åŠ åˆ°ç¼“å†²åŒº
            if self.augment:
                self.ims[i], self.im_hw0[i], self.im_hw[i] = im, (h0, w0), im.shape[:2]  # im, åŸå§‹å°ºå¯¸, è°ƒæ•´åå°ºå¯¸
                self.buffer.append(i)  # å°†ç´¢å¼•æ·»åŠ åˆ°ç¼“å†²åŒº
                if 1 < len(self.buffer) >= self.max_buffer_length:  # é˜²æ­¢ç¼“å†²åŒºä¸ºç©º
                    j = self.buffer.pop(0)  # ç§»é™¤æœ€æ—§çš„ç´¢å¼•
                    if self.cache != "ram":
                        self.ims[j], self.im_hw0[j], self.im_hw[j] = None, None, None  # æ¸…ç† RAM ä¸­çš„ç¼“å­˜

            return im, (h0, w0), im.shape[:2]  # è¿”å›å›¾åƒå’Œå°ºå¯¸ä¿¡æ¯

        return self.ims[i], self.im_hw0[i], self.im_hw[i]  # å¦‚æœå·²ç¼“å­˜ï¼Œè¿”å›ç¼“å­˜çš„å›¾åƒå’Œå°ºå¯¸ä¿¡æ¯

    def cache_images(self):
        """å°†å›¾åƒç¼“å­˜åˆ°å†…å­˜æˆ–ç£ç›˜ã€‚"""
        b, gb = 0, 1 << 30  # ç¼“å­˜å›¾åƒçš„å­—èŠ‚æ•°ï¼Œ1 GB çš„å­—èŠ‚æ•°
        fcn, storage = (self.cache_images_to_disk, "Disk") if self.cache == "disk" else (
            self.load_image, "RAM")  # æ ¹æ®ç¼“å­˜ç±»å‹é€‰æ‹©å‡½æ•°
        with ThreadPool(NUM_THREADS) as pool:  # ä½¿ç”¨çº¿ç¨‹æ± 
            results = pool.imap(fcn, range(self.ni))  # å¹¶è¡ŒåŠ è½½å›¾åƒ
            pbar = TQDM(enumerate(results), total=self.ni)  # åˆ›å»ºè¿›åº¦æ¡
            for i, x in pbar:
                if self.cache == "disk":
                    b += self.npy_files[i].stat().st_size  # ç´¯åŠ ç£ç›˜ç¼“å­˜å¤§å°
                else:  # 'ram'
                    self.ims[i], self.im_hw0[i], self.im_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)
                    b += self.ims[i].nbytes  # ç´¯åŠ  RAM ç¼“å­˜å¤§å°
                pbar.desc = f"{self.prefix}ç¼“å­˜å›¾åƒ ({b / gb:.1f}GB {storage})"  # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.close()  # å…³é—­è¿›åº¦æ¡

    def cache_labels(self, path=Path("./labels.cache")):
        """
        ç¼“å­˜æ•°æ®é›†æ ‡ç­¾ï¼Œæ£€æŸ¥å›¾åƒå¹¶è¯»å–å½¢çŠ¶ã€‚

        å‚æ•°ï¼š
            path (Path): ä¿å­˜ç¼“å­˜æ–‡ä»¶çš„è·¯å¾„ã€‚é»˜è®¤ä¸º Path('./labels.cache')ã€‚

        è¿”å›ï¼š
            (dict): æ ‡ç­¾ä¿¡æ¯ã€‚
        """
        x = {"labels": []}  # åˆå§‹åŒ–æ ‡ç­¾å­—å…¸
        nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # åˆå§‹åŒ–è®¡æ•°å™¨
        desc = f"{self.prefix}æ‰«æ {path.parent / path.stem}..."
        total = len(self.im_files)  # æ•°æ®é›†ä¸­å›¾åƒçš„æ€»æ•°
        nkpt, ndim = self.data.get("kpt_shape", (0, 0))  # è·å–å…³é”®ç‚¹å½¢çŠ¶
        with ThreadPool(NUM_THREADS) as pool:  # ä½¿ç”¨çº¿ç¨‹æ± 
            results = pool.imap(
                func=verify_image_label,  # éªŒè¯æ¯ä¸ªå›¾åƒçš„æ ‡ç­¾
                iterable=zip(
                    self.im_files,
                    self.label_files,
                    repeat(self.prefix),
                    repeat(len(self.data["names"])),
                    repeat(nkpt),
                    repeat(ndim),
                ),
            )
            pbar = TQDM(results, desc=desc, total=total)  # åˆ›å»ºè¿›åº¦æ¡
            for im_file, lb, shape, segments, keypoint, nm_f, nf_f, ne_f, nc_f, msg in pbar:
                nm += nm_f  # æ›´æ–°ç¼ºå¤±è®¡æ•°
                nf += nf_f  # æ›´æ–°æ‰¾åˆ°è®¡æ•°
                ne += ne_f  # æ›´æ–°ç©ºè®¡æ•°
                nc += nc_f  # æ›´æ–°æŸåè®¡æ•°
                if im_file:
                    x["labels"].append(
                        {
                            "im_file": im_file,  # å›¾åƒæ–‡ä»¶è·¯å¾„
                            "shape": shape,  # å›¾åƒå½¢çŠ¶
                            "cls": lb[:, 0:1],  # ç±»åˆ«
                            "bboxes": lb[:, 1:],  # è¾¹ç•Œæ¡†
                            "segments": segments,  # åˆ†æ®µ
                            "keypoints": keypoint,  # å…³é”®ç‚¹
                            "normalized": True,  # æ˜¯å¦å½’ä¸€åŒ–
                            "bbox_format": "xywh",  # è¾¹ç•Œæ¡†æ ¼å¼
                        }
                    )
                if msg:
                    msgs.append(msg)  # æ”¶é›†æ¶ˆæ¯
                pbar.desc = f"{desc} {nf} å›¾åƒ, {nm + ne} èƒŒæ™¯, {nc} æŸå"  # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.close()  # å…³é—­è¿›åº¦æ¡

        if msgs:
            LOGGER.info("\n".join(msgs))  # æ‰“å°æ¶ˆæ¯
        if nf == 0:
            LOGGER.warning(f"{self.prefix}è­¦å‘Š âš ï¸ åœ¨ {path} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ ‡ç­¾ã€‚ {HELP_URL}")
        x["hash"] = get_hash(self.label_files + self.im_files)  # è®¡ç®—å“ˆå¸Œå€¼
        x["results"] = nf, nm, ne, nc, len(self.im_files)  # æ›´æ–°ç»“æœä¿¡æ¯
        x["msgs"] = msgs  # æ”¶é›†è­¦å‘Šæ¶ˆæ¯
        save_dataset_cache_file(self.prefix, path, x, DATASET_CACHE_VERSION)  # ä¿å­˜ç¼“å­˜æ–‡ä»¶
        return x  # è¿”å›æ ‡ç­¾ä¿¡æ¯

    def cache_images_to_disk(self, i):
        """å°†å›¾åƒä¿å­˜ä¸º *.npy æ–‡ä»¶ä»¥åŠ å¿«åŠ è½½é€Ÿåº¦ã€‚"""
        f = self.npy_files[i]  # è·å– Numpy æ–‡ä»¶è·¯å¾„
        if not f.exists():  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
            np.save(f.as_posix(), cv2.imread(self.im_files[i]), allow_pickle=False)  # ä¿å­˜å›¾åƒä¸º Numpy æ–‡ä»¶

    def check_cache_disk(self, safety_margin=0.5):
        """æ£€æŸ¥å›¾åƒç¼“å­˜éœ€æ±‚ä¸å¯ç”¨ç£ç›˜ç©ºé—´çš„å…³ç³»ã€‚"""
        import shutil

        b, gb = 0, 1 << 30  # ç¼“å­˜å›¾åƒçš„å­—èŠ‚æ•°ï¼Œ1 GB çš„å­—èŠ‚æ•°
        n = min(self.ni, 30)  # ä» 30 å¼ éšæœºå›¾åƒæ¨æ–­
        for _ in range(n):
            im_file = random.choice(self.im_files)  # éšæœºé€‰æ‹©ä¸€å¼ å›¾åƒ
            im = cv2.imread(im_file)  # è¯»å–å›¾åƒ
            if im is None:
                continue
            b += im.nbytes  # ç´¯åŠ å›¾åƒå­—èŠ‚æ•°
            if not os.access(Path(im_file).parent, os.W_OK):  # æ£€æŸ¥ç›®å½•æ˜¯å¦å¯å†™
                self.cache = None
                LOGGER.info(f"{self.prefix}è·³è¿‡å°†å›¾åƒç¼“å­˜åˆ°ç£ç›˜ï¼Œç›®å½•ä¸å¯å†™ âš ï¸")
                return False
        disk_required = b * self.ni / n * (1 + safety_margin)  # è®¡ç®—ç¼“å­˜æ•°æ®é›†åˆ°ç£ç›˜æ‰€éœ€çš„å­—èŠ‚æ•°
        total, used, free = shutil.disk_usage(Path(self.im_files[0]).parent)  # è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µ
        if disk_required > free:  # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿç©ºé—´
            self.cache = None
            LOGGER.info(
                f"{self.prefix}{disk_required / gb:.1f}GB ç£ç›˜ç©ºé—´éœ€æ±‚ï¼Œ"
                f"å®‰å…¨è¾¹é™… {int(safety_margin * 100)}% ä½†åªæœ‰ "
                f"{free / gb:.1f}/{total / gb:.1f}GB å¯ç”¨ï¼Œæœªç¼“å­˜å›¾åƒåˆ°ç£ç›˜ âš ï¸"
            )
            return False
        return True  # ç£ç›˜ç©ºé—´è¶³å¤Ÿï¼Œè¿”å› True

    def check_cache_ram(self, safety_margin=0.5):
        """æ£€æŸ¥å›¾åƒç¼“å­˜éœ€æ±‚ä¸å¯ç”¨å†…å­˜çš„å…³ç³»ã€‚"""
        b, gb = 0, 1 << 30  # ç¼“å­˜å›¾åƒçš„å­—èŠ‚æ•°ï¼Œ1 GB çš„å­—èŠ‚æ•°
        n = min(self.ni, 30)  # ä» 30 å¼ éšæœºå›¾åƒæ¨æ–­
        for _ in range(n):
            im = cv2.imread(random.choice(self.im_files))  # è¯»å–éšæœºå›¾åƒ
            if im is None:
                continue
            ratio = self.imgsz / max(im.shape[0], im.shape[1])  # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            b += im.nbytes * ratio ** 2  # è®¡ç®—å›¾åƒç¼“å­˜æ‰€éœ€çš„å­—èŠ‚æ•°
        mem_required = b * self.ni / n * (1 + safety_margin)  # è®¡ç®—ç¼“å­˜æ•°æ®é›†åˆ° RAM æ‰€éœ€çš„å­—èŠ‚æ•°
        mem = psutil.virtual_memory()  # è·å–è™šæ‹Ÿå†…å­˜ä¿¡æ¯
        if mem_required > mem.available:  # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å¯ç”¨å†…å­˜
            self.cache = None
            LOGGER.info(
                f"{self.prefix}{mem_required / gb:.1f}GB RAM éœ€æ±‚ç¼“å­˜å›¾åƒï¼Œ"
                f"å®‰å…¨è¾¹é™… {int(safety_margin * 100)}% ä½†åªæœ‰ "
                f"{mem.available / gb:.1f}/{mem.total / gb:.1f}GB å¯ç”¨ï¼Œæœªç¼“å­˜å›¾åƒ âš ï¸"
            )
            return False  # å†…å­˜ä¸è¶³ï¼Œè¿”å› False
        return True  # å†…å­˜è¶³å¤Ÿï¼Œè¿”å› True

    def set_rectangle(self):
        """è®¾ç½® YOLO æ£€æµ‹çš„è¾¹ç•Œæ¡†å½¢çŠ¶ä¸ºçŸ©å½¢ã€‚"""
        bi = np.floor(np.arange(self.ni) / self.batch_size).astype(int)  # æ‰¹æ¬¡ç´¢å¼•
        nb = bi[-1] + 1  # æ‰¹æ¬¡æ•°

        s = np.array([x.pop("shape") for x in self.labels])  # è·å–å›¾åƒåŸå§‹å½¢çŠ¶
        ar = s[:, 0] / s[:, 1]  # è®¡ç®—é•¿å®½æ¯”
        irect = ar.argsort()  # æŒ‰é•¿å®½æ¯”æ’åº
        self.im_files = [self.im_files[i] for i in irect]  # æ ¹æ®æ’åºæ›´æ–°å›¾åƒæ–‡ä»¶åˆ—è¡¨
        self.labels = [self.labels[i] for i in irect]  # æ ¹æ®æ’åºæ›´æ–°æ ‡ç­¾åˆ—è¡¨
        ar = ar[irect]  # æ›´æ–°é•¿å®½æ¯”

        # è®¾ç½®è®­ç»ƒå›¾åƒçš„å½¢çŠ¶
        shapes = [[1, 1]] * nb  # åˆå§‹åŒ–å½¢çŠ¶
        for i in range(nb):
            ari = ar[bi == i]  # è·å–å½“å‰æ‰¹æ¬¡çš„é•¿å®½æ¯”
            mini, maxi = ari.min(), ari.max()  # è·å–å½“å‰æ‰¹æ¬¡çš„æœ€å°å’Œæœ€å¤§é•¿å®½æ¯”
            if maxi < 1:
                shapes[i] = [maxi, 1]  # å¦‚æœæœ€å¤§æ¯”å€¼å°äº 1
            elif mini > 1:
                shapes[i] = [1, 1 / mini]  # å¦‚æœæœ€å°æ¯”å€¼å¤§äº 1

        # è®¡ç®—æ‰¹æ¬¡å½¢çŠ¶
        self.batch_shapes = np.ceil(np.array(shapes) * self.imgsz / self.stride + self.pad).astype(int) * self.stride
        self.batch = bi  # æ›´æ–°å›¾åƒçš„æ‰¹æ¬¡ç´¢å¼•

    def __getitem__(self, index):
        """è¿”å›æŒ‡å®šç´¢å¼•çš„æ ‡ç­¾ä¿¡æ¯çš„è½¬æ¢ç»“æœã€‚"""
        return self.transforms(self.get_image_and_label(index))  # åº”ç”¨è½¬æ¢å¹¶è¿”å›ç»“æœ

    def get_image_and_label(self, index):
        """è·å–å¹¶è¿”å›æ•°æ®é›†ä¸­çš„å›¾åƒå’Œæ ‡ç­¾ä¿¡æ¯ã€‚"""
        label = deepcopy(self.labels[index])  # æ·±æ‹·è´æ ‡ç­¾ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        label.pop("shape", None)  # ç§»é™¤å½¢çŠ¶ä¿¡æ¯
        label["img"], label["ori_shape"], label["resized_shape"] = self.load_image(index)  # åŠ è½½å›¾åƒåŠå…¶å½¢çŠ¶
        label["ratio_pad"] = (
            label["resized_shape"][0] / label["ori_shape"][0],
            label["resized_shape"][1] / label["ori_shape"][1],
        )  # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        if self.rect:  # å¦‚æœä½¿ç”¨çŸ©å½¢è®­ç»ƒ
            label["rect_shape"] = self.batch_shapes[self.batch[index]]  # è·å–å½“å‰å›¾åƒçš„çŸ©å½¢å½¢çŠ¶
        return self.update_labels_info(label)  # æ›´æ–°æ ‡ç­¾ä¿¡æ¯å¹¶è¿”å›

    def __len__(self):
        """è¿”å›æ•°æ®é›†ä¸­æ ‡ç­¾çš„æ•°é‡ã€‚"""
        return len(self.labels)  # è¿”å›æ ‡ç­¾æ•°é‡

    def update_labels_info(self, label):
        """
        è‡ªå®šä¹‰æ ‡ç­¾æ ¼å¼ã€‚

        æ³¨æ„ï¼š
            cls ç°åœ¨ä¸ä¸è¾¹ç•Œæ¡†ä¸€èµ·å­˜åœ¨ï¼Œåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²éœ€è¦ç‹¬ç«‹çš„ cls æ ‡ç­¾ã€‚
            é€šè¿‡æ·»åŠ æˆ–åˆ é™¤å­—å…¸é”®ï¼Œä¹Ÿå¯ä»¥æ”¯æŒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ã€‚
        """
        bboxes = label.pop("bboxes")  # è·å–å¹¶ç§»é™¤è¾¹ç•Œæ¡†ä¿¡æ¯
        segments = label.pop("segments", [])  # è·å–å¹¶ç§»é™¤åˆ†æ®µä¿¡æ¯
        keypoints = label.pop("keypoints", None)  # è·å–å¹¶ç§»é™¤å…³é”®ç‚¹ä¿¡æ¯
        bbox_format = label.pop("bbox_format")  # è·å–å¹¶ç§»é™¤è¾¹ç•Œæ¡†æ ¼å¼
        normalized = label.pop("normalized")  # è·å–å¹¶ç§»é™¤å½’ä¸€åŒ–æ ‡å¿—

        # æ³¨æ„ï¼šä¸å¯¹æœ‰å‘è¾¹ç•Œæ¡†è¿›è¡Œé‡é‡‡æ ·
        segment_resamples =  1000  # è®¾ç½®åˆ†æ®µé‡é‡‡æ ·æ•°
        if len(segments) > 0:  # å¦‚æœæœ‰åˆ†æ®µä¿¡æ¯
            # list[np.array(1000, 2)] * num_samples
            # (N, 1000, 2)
            segments = np.stack(resample_segments(segments, n=segment_resamples), axis=0)  # è¿›è¡Œé‡é‡‡æ ·
        else:
            segments = np.zeros((0, segment_resamples, 2), dtype=np.float32)  # å¦‚æœæ²¡æœ‰åˆ†æ®µï¼Œè¿”å›é›¶æ•°ç»„
        label["instances"] = Instances(bboxes, segments, keypoints, bbox_format=bbox_format,
                                       normalized=normalized)  # æ›´æ–°å®ä¾‹ä¿¡æ¯
        return label  # è¿”å›æ›´æ–°åçš„æ ‡ç­¾

    def build_transforms(self, hyp=None):
        """æ„å»ºå›¾åƒè½¬æ¢å‡½æ•°ã€‚"""
        if self.augment:  # å¦‚æœä½¿ç”¨æ•°æ®å¢å¼º
            hyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0  # è®¾ç½®é©¬èµ›å…‹æ¯”ä¾‹
            hyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0  # è®¾ç½®æ··åˆæ¯”ä¾‹
            transforms = v8_transforms(self, self.imgsz, hyp)  # æ„å»ºæ•°æ®å¢å¼ºè½¬æ¢
        else:
            transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False)])  # å¦‚æœä¸ä½¿ç”¨å¢å¼ºï¼Œè®¾ç½®ä¸ºå›ºå®šå¤§å°
        transforms.append(
            Format(
                bbox_format="xywh",  # è®¾ç½®è¾¹ç•Œæ¡†æ ¼å¼
                normalize=True,  # æ˜¯å¦å½’ä¸€åŒ–
                batch_idx=True,  # æ˜¯å¦è¿”å›æ‰¹æ¬¡ç´¢å¼•
                bgr=hyp.bgr if self.augment else 0.0,  # ä»…å½±å“è®­ç»ƒ
            )
        )
        return transforms  # è¿”å›æ„å»ºå¥½çš„è½¬æ¢

    def get_labels(self):
        self.label_files = img2label_paths(self.im_files)  # è·å–æ ‡ç­¾æ–‡ä»¶è·¯å¾„
        cache_path = Path(self.label_files[0]).parent.with_suffix(".cache")  # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        try:
            cache, exists = load_dataset_cache_file(cache_path), True  # å°è¯•åŠ è½½ç¼“å­˜æ–‡ä»¶
            assert cache["version"] == DATASET_CACHE_VERSION  # ç¡®ä¿ç‰ˆæœ¬åŒ¹é…
            assert cache["hash"] == get_hash(self.label_files + self.im_files)  # ç¡®ä¿å“ˆå¸Œå€¼åŒ¹é…
        except (FileNotFoundError, AssertionError, AttributeError):
            cache, exists = self.cache_labels(cache_path), False  # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¿è¡Œç¼“å­˜æ“ä½œ

        # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
        nf, nm, ne, nc, n = cache.pop("results")  # æ‰¾åˆ°çš„ã€ç¼ºå¤±çš„ã€ç©ºçš„ã€æŸåçš„å›¾åƒæ•°é‡
        if exists:
            d = f"æ‰«æ {cache_path}... {nf} å›¾åƒ, {nm + ne} èƒŒæ™¯, {nc} æŸå"
            TQDM(None, desc=self.prefix + d, total=n, initial=n)  # æ˜¾ç¤ºç»“æœ
            if cache["msgs"]:
                LOGGER.info("\n".join(cache["msgs"]))  # æ‰“å°è­¦å‘Šä¿¡æ¯

        # è¯»å–ç¼“å­˜
        [cache.pop(k) for k in ("hash", "version", "msgs")]  # ç§»é™¤æ— ç”¨é¡¹
        labels = cache["labels"]  # è·å–æ ‡ç­¾ä¿¡æ¯
        if not labels:
            LOGGER.warning(f"è­¦å‘Š âš ï¸ åœ¨ {cache_path} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ ‡ç­¾ï¼Œè®­ç»ƒå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")
        self.im_files = [lb["im_file"] for lb in labels]  # æ›´æ–°å›¾åƒæ–‡ä»¶è·¯å¾„
        return labels  # è¿”å›æ ‡ç­¾ä¿¡æ¯

    def close_mosaic(self, hyp):
        """å°†é©¬èµ›å…‹ã€å¤åˆ¶ç²˜è´´å’Œæ··åˆé€‰é¡¹è®¾ç½®ä¸º 0.0 å¹¶æ„å»ºè½¬æ¢ã€‚"""
        hyp.mosaic = 0.0  # è®¾ç½®é©¬èµ›å…‹æ¯”ä¾‹ä¸º 0.0
        hyp.copy_paste = 0.0  # ä¿æŒä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´
        hyp.mixup = 0.0  # ä¿æŒä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´
        self.transforms = self.build_transforms(hyp)  # æ›´æ–°è½¬æ¢

    @staticmethod
    def collate_fn(batch):
        """å°†æ•°æ®æ ·æœ¬åˆå¹¶æˆæ‰¹æ¬¡ã€‚"""
        new_batch = {}
        keys = batch[0].keys()  # è·å–æ‰¹æ¬¡ä¸­çš„é”®
        values = list(zip(*[list(b.values()) for b in batch]))  # å°†æ¯ä¸ªæ ·æœ¬çš„å€¼æŒ‰é”®åˆå¹¶
        for i, k in enumerate(keys):
            value = values[i]  # è·å–å½“å‰é”®çš„å€¼
            if k == "img":
                value = torch.stack(value, 0)  # å †å å›¾åƒ
            if k in {"masks", "keypoints", "bboxes", "cls", "segments", "obb"}:
                value = torch.cat(value, 0)  # åˆå¹¶æ©ç ã€å…³é”®ç‚¹ã€è¾¹ç•Œæ¡†ã€ç±»åˆ«ã€åˆ†æ®µç­‰
            new_batch[k] = value  # æ›´æ–°åˆå¹¶åçš„æ‰¹æ¬¡

        new_batch["batch_idx"] = list(new_batch["batch_idx"])  # è·å–æ‰¹æ¬¡ç´¢å¼•
        for i in range(len(new_batch["batch_idx"])):
            new_batch["batch_idx"][i] += i  # ä¸º build_targets() æ·»åŠ ç›®æ ‡å›¾åƒç´¢å¼•
        new_batch["batch_idx"] = torch.cat(new_batch["batch_idx"], 0)  # åˆå¹¶ç´¢å¼•
        return new_batch  # è¿”å›åˆå¹¶åçš„æ‰¹æ¬¡
